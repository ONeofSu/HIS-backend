{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.3928647573022528,
  "eval_steps": 500,
  "global_step": 9800,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0028427261744012507,
      "grad_norm": 0.7378877997398376,
      "learning_rate": 0.0002997299602046617,
      "loss": 1.2585,
      "step": 20
    },
    {
      "epoch": 0.005685452348802501,
      "grad_norm": 0.7573286890983582,
      "learning_rate": 0.0002994457077885162,
      "loss": 1.0135,
      "step": 40
    },
    {
      "epoch": 0.008528178523203752,
      "grad_norm": 0.692326009273529,
      "learning_rate": 0.00029916145537237066,
      "loss": 0.9268,
      "step": 60
    },
    {
      "epoch": 0.011370904697605003,
      "grad_norm": 0.744557797908783,
      "learning_rate": 0.0002988772029562251,
      "loss": 0.9077,
      "step": 80
    },
    {
      "epoch": 0.014213630872006253,
      "grad_norm": 0.7190642952919006,
      "learning_rate": 0.00029859295054007957,
      "loss": 0.8611,
      "step": 100
    },
    {
      "epoch": 0.017056357046407504,
      "grad_norm": 0.6957486867904663,
      "learning_rate": 0.00029830869812393405,
      "loss": 0.8372,
      "step": 120
    },
    {
      "epoch": 0.019899083220808755,
      "grad_norm": 0.770143985748291,
      "learning_rate": 0.00029802444570778847,
      "loss": 0.8172,
      "step": 140
    },
    {
      "epoch": 0.022741809395210005,
      "grad_norm": 0.6843509078025818,
      "learning_rate": 0.00029774019329164295,
      "loss": 0.8062,
      "step": 160
    },
    {
      "epoch": 0.025584535569611256,
      "grad_norm": 0.7450367212295532,
      "learning_rate": 0.0002974559408754974,
      "loss": 0.7817,
      "step": 180
    },
    {
      "epoch": 0.028427261744012507,
      "grad_norm": 0.7087247967720032,
      "learning_rate": 0.0002971716884593519,
      "loss": 0.767,
      "step": 200
    },
    {
      "epoch": 0.03126998791841376,
      "grad_norm": 0.6939836740493774,
      "learning_rate": 0.00029688743604320634,
      "loss": 0.7668,
      "step": 220
    },
    {
      "epoch": 0.03411271409281501,
      "grad_norm": 0.7414717674255371,
      "learning_rate": 0.0002966031836270608,
      "loss": 0.7599,
      "step": 240
    },
    {
      "epoch": 0.03695544026721626,
      "grad_norm": 0.6676125526428223,
      "learning_rate": 0.0002963189312109153,
      "loss": 0.7444,
      "step": 260
    },
    {
      "epoch": 0.03979816644161751,
      "grad_norm": 0.751083493232727,
      "learning_rate": 0.0002960346787947697,
      "loss": 0.727,
      "step": 280
    },
    {
      "epoch": 0.04264089261601876,
      "grad_norm": 0.6737906336784363,
      "learning_rate": 0.0002957504263786242,
      "loss": 0.731,
      "step": 300
    },
    {
      "epoch": 0.04548361879042001,
      "grad_norm": 0.6945453882217407,
      "learning_rate": 0.0002954661739624787,
      "loss": 0.7226,
      "step": 320
    },
    {
      "epoch": 0.048326344964821265,
      "grad_norm": 0.7906206250190735,
      "learning_rate": 0.0002951819215463331,
      "loss": 0.7117,
      "step": 340
    },
    {
      "epoch": 0.05116907113922251,
      "grad_norm": 0.6844779849052429,
      "learning_rate": 0.0002948976691301876,
      "loss": 0.693,
      "step": 360
    },
    {
      "epoch": 0.054011797313623766,
      "grad_norm": 0.6598177552223206,
      "learning_rate": 0.000294613416714042,
      "loss": 0.7091,
      "step": 380
    },
    {
      "epoch": 0.05685452348802501,
      "grad_norm": 0.7142188549041748,
      "learning_rate": 0.0002943291642978965,
      "loss": 0.6966,
      "step": 400
    },
    {
      "epoch": 0.05969724966242627,
      "grad_norm": 0.6866647005081177,
      "learning_rate": 0.00029404491188175097,
      "loss": 0.6871,
      "step": 420
    },
    {
      "epoch": 0.06253997583682752,
      "grad_norm": 0.7114967703819275,
      "learning_rate": 0.0002937606594656054,
      "loss": 0.6908,
      "step": 440
    },
    {
      "epoch": 0.06538270201122877,
      "grad_norm": 0.6430743336677551,
      "learning_rate": 0.00029347640704945993,
      "loss": 0.6934,
      "step": 460
    },
    {
      "epoch": 0.06822542818563002,
      "grad_norm": 0.6902050375938416,
      "learning_rate": 0.00029319215463331435,
      "loss": 0.6697,
      "step": 480
    },
    {
      "epoch": 0.07106815436003126,
      "grad_norm": 0.7060803771018982,
      "learning_rate": 0.00029290790221716883,
      "loss": 0.6724,
      "step": 500
    },
    {
      "epoch": 0.07391088053443252,
      "grad_norm": 0.7220724821090698,
      "learning_rate": 0.0002926236498010233,
      "loss": 0.6824,
      "step": 520
    },
    {
      "epoch": 0.07675360670883377,
      "grad_norm": 0.748457133769989,
      "learning_rate": 0.00029233939738487774,
      "loss": 0.6653,
      "step": 540
    },
    {
      "epoch": 0.07959633288323502,
      "grad_norm": 0.6238874793052673,
      "learning_rate": 0.0002920551449687322,
      "loss": 0.6742,
      "step": 560
    },
    {
      "epoch": 0.08243905905763628,
      "grad_norm": 0.7251567840576172,
      "learning_rate": 0.0002917708925525867,
      "loss": 0.6719,
      "step": 580
    },
    {
      "epoch": 0.08528178523203753,
      "grad_norm": 0.6483393907546997,
      "learning_rate": 0.0002914866401364411,
      "loss": 0.6535,
      "step": 600
    },
    {
      "epoch": 0.08812451140643877,
      "grad_norm": 0.7014157176017761,
      "learning_rate": 0.0002912023877202956,
      "loss": 0.6735,
      "step": 620
    },
    {
      "epoch": 0.09096723758084002,
      "grad_norm": 0.6925122141838074,
      "learning_rate": 0.00029091813530415003,
      "loss": 0.6481,
      "step": 640
    },
    {
      "epoch": 0.09380996375524128,
      "grad_norm": 0.7249041199684143,
      "learning_rate": 0.0002906338828880045,
      "loss": 0.6597,
      "step": 660
    },
    {
      "epoch": 0.09665268992964253,
      "grad_norm": 0.6367669105529785,
      "learning_rate": 0.000290349630471859,
      "loss": 0.6393,
      "step": 680
    },
    {
      "epoch": 0.09949541610404378,
      "grad_norm": 0.7272034287452698,
      "learning_rate": 0.0002900653780557134,
      "loss": 0.6616,
      "step": 700
    },
    {
      "epoch": 0.10233814227844502,
      "grad_norm": 0.7186036705970764,
      "learning_rate": 0.00028978112563956795,
      "loss": 0.6408,
      "step": 720
    },
    {
      "epoch": 0.10518086845284629,
      "grad_norm": 0.6629007458686829,
      "learning_rate": 0.00028949687322342237,
      "loss": 0.6534,
      "step": 740
    },
    {
      "epoch": 0.10802359462724753,
      "grad_norm": 0.65947425365448,
      "learning_rate": 0.00028921262080727685,
      "loss": 0.6292,
      "step": 760
    },
    {
      "epoch": 0.11086632080164878,
      "grad_norm": 0.6891085505485535,
      "learning_rate": 0.00028892836839113133,
      "loss": 0.6514,
      "step": 780
    },
    {
      "epoch": 0.11370904697605003,
      "grad_norm": 0.6088302135467529,
      "learning_rate": 0.00028864411597498576,
      "loss": 0.6408,
      "step": 800
    },
    {
      "epoch": 0.11655177315045129,
      "grad_norm": 0.6525115966796875,
      "learning_rate": 0.00028835986355884024,
      "loss": 0.627,
      "step": 820
    },
    {
      "epoch": 0.11939449932485253,
      "grad_norm": 0.6607556939125061,
      "learning_rate": 0.00028807561114269466,
      "loss": 0.6239,
      "step": 840
    },
    {
      "epoch": 0.12223722549925378,
      "grad_norm": 0.6493237614631653,
      "learning_rate": 0.00028779135872654914,
      "loss": 0.6457,
      "step": 860
    },
    {
      "epoch": 0.12507995167365504,
      "grad_norm": 0.6894469857215881,
      "learning_rate": 0.0002875071063104036,
      "loss": 0.6405,
      "step": 880
    },
    {
      "epoch": 0.12792267784805628,
      "grad_norm": 0.7013271450996399,
      "learning_rate": 0.00028722285389425804,
      "loss": 0.6483,
      "step": 900
    },
    {
      "epoch": 0.13076540402245754,
      "grad_norm": 0.6578665375709534,
      "learning_rate": 0.0002869386014781125,
      "loss": 0.6269,
      "step": 920
    },
    {
      "epoch": 0.1336081301968588,
      "grad_norm": 0.6606793403625488,
      "learning_rate": 0.000286654349061967,
      "loss": 0.6179,
      "step": 940
    },
    {
      "epoch": 0.13645085637126003,
      "grad_norm": 0.6278192400932312,
      "learning_rate": 0.00028637009664582143,
      "loss": 0.6329,
      "step": 960
    },
    {
      "epoch": 0.1392935825456613,
      "grad_norm": 0.6440424919128418,
      "learning_rate": 0.00028608584422967596,
      "loss": 0.626,
      "step": 980
    },
    {
      "epoch": 0.14213630872006253,
      "grad_norm": 0.6192002892494202,
      "learning_rate": 0.0002858015918135304,
      "loss": 0.6235,
      "step": 1000
    },
    {
      "epoch": 0.1449790348944638,
      "grad_norm": 0.5617451667785645,
      "learning_rate": 0.00028551733939738487,
      "loss": 0.5966,
      "step": 1020
    },
    {
      "epoch": 0.14782176106886505,
      "grad_norm": 0.6651932001113892,
      "learning_rate": 0.0002852330869812393,
      "loss": 0.6137,
      "step": 1040
    },
    {
      "epoch": 0.15066448724326628,
      "grad_norm": 0.7178025841712952,
      "learning_rate": 0.0002849488345650938,
      "loss": 0.6195,
      "step": 1060
    },
    {
      "epoch": 0.15350721341766754,
      "grad_norm": 0.6457552313804626,
      "learning_rate": 0.00028466458214894825,
      "loss": 0.6271,
      "step": 1080
    },
    {
      "epoch": 0.1563499395920688,
      "grad_norm": 0.6539117097854614,
      "learning_rate": 0.0002843803297328027,
      "loss": 0.628,
      "step": 1100
    },
    {
      "epoch": 0.15919266576647004,
      "grad_norm": 0.6435642242431641,
      "learning_rate": 0.00028409607731665716,
      "loss": 0.6109,
      "step": 1120
    },
    {
      "epoch": 0.1620353919408713,
      "grad_norm": 0.6299681067466736,
      "learning_rate": 0.00028381182490051164,
      "loss": 0.6352,
      "step": 1140
    },
    {
      "epoch": 0.16487811811527256,
      "grad_norm": 0.6073504090309143,
      "learning_rate": 0.00028352757248436606,
      "loss": 0.6252,
      "step": 1160
    },
    {
      "epoch": 0.1677208442896738,
      "grad_norm": 0.6489642262458801,
      "learning_rate": 0.0002832433200682206,
      "loss": 0.6105,
      "step": 1180
    },
    {
      "epoch": 0.17056357046407505,
      "grad_norm": 0.6632421612739563,
      "learning_rate": 0.000282959067652075,
      "loss": 0.6043,
      "step": 1200
    },
    {
      "epoch": 0.1734062966384763,
      "grad_norm": 0.6185342669487,
      "learning_rate": 0.0002826748152359295,
      "loss": 0.6188,
      "step": 1220
    },
    {
      "epoch": 0.17624902281287755,
      "grad_norm": 0.6178563237190247,
      "learning_rate": 0.0002823905628197839,
      "loss": 0.6175,
      "step": 1240
    },
    {
      "epoch": 0.1790917489872788,
      "grad_norm": 0.6528903245925903,
      "learning_rate": 0.0002821063104036384,
      "loss": 0.6133,
      "step": 1260
    },
    {
      "epoch": 0.18193447516168004,
      "grad_norm": 0.6289127469062805,
      "learning_rate": 0.0002818220579874929,
      "loss": 0.5967,
      "step": 1280
    },
    {
      "epoch": 0.1847772013360813,
      "grad_norm": 0.6468214392662048,
      "learning_rate": 0.0002815378055713473,
      "loss": 0.6174,
      "step": 1300
    },
    {
      "epoch": 0.18761992751048256,
      "grad_norm": 0.6428674459457397,
      "learning_rate": 0.0002812535531552018,
      "loss": 0.592,
      "step": 1320
    },
    {
      "epoch": 0.1904626536848838,
      "grad_norm": 0.7012666463851929,
      "learning_rate": 0.00028096930073905627,
      "loss": 0.6015,
      "step": 1340
    },
    {
      "epoch": 0.19330537985928506,
      "grad_norm": 0.6000802516937256,
      "learning_rate": 0.0002806850483229107,
      "loss": 0.6071,
      "step": 1360
    },
    {
      "epoch": 0.1961481060336863,
      "grad_norm": 0.6136250495910645,
      "learning_rate": 0.0002804007959067652,
      "loss": 0.5985,
      "step": 1380
    },
    {
      "epoch": 0.19899083220808755,
      "grad_norm": 0.7135531306266785,
      "learning_rate": 0.00028011654349061966,
      "loss": 0.6067,
      "step": 1400
    },
    {
      "epoch": 0.20183355838248881,
      "grad_norm": 0.7148944735527039,
      "learning_rate": 0.0002798322910744741,
      "loss": 0.5763,
      "step": 1420
    },
    {
      "epoch": 0.20467628455689005,
      "grad_norm": 0.614195704460144,
      "learning_rate": 0.00027954803865832856,
      "loss": 0.5927,
      "step": 1440
    },
    {
      "epoch": 0.2075190107312913,
      "grad_norm": 0.626223087310791,
      "learning_rate": 0.00027926378624218304,
      "loss": 0.5929,
      "step": 1460
    },
    {
      "epoch": 0.21036173690569257,
      "grad_norm": 0.6033298373222351,
      "learning_rate": 0.0002789795338260375,
      "loss": 0.6011,
      "step": 1480
    },
    {
      "epoch": 0.2132044630800938,
      "grad_norm": 0.5800248980522156,
      "learning_rate": 0.00027869528140989194,
      "loss": 0.5752,
      "step": 1500
    },
    {
      "epoch": 0.21604718925449506,
      "grad_norm": 0.6173252463340759,
      "learning_rate": 0.0002784110289937464,
      "loss": 0.5818,
      "step": 1520
    },
    {
      "epoch": 0.2188899154288963,
      "grad_norm": 0.6504303812980652,
      "learning_rate": 0.0002781267765776009,
      "loss": 0.6095,
      "step": 1540
    },
    {
      "epoch": 0.22173264160329756,
      "grad_norm": 0.6230430603027344,
      "learning_rate": 0.00027784252416145533,
      "loss": 0.6064,
      "step": 1560
    },
    {
      "epoch": 0.22457536777769882,
      "grad_norm": 0.5766837000846863,
      "learning_rate": 0.0002775582717453098,
      "loss": 0.5876,
      "step": 1580
    },
    {
      "epoch": 0.22741809395210005,
      "grad_norm": 0.6247766017913818,
      "learning_rate": 0.0002772740193291643,
      "loss": 0.5825,
      "step": 1600
    },
    {
      "epoch": 0.23026082012650131,
      "grad_norm": 0.582248330116272,
      "learning_rate": 0.0002769897669130187,
      "loss": 0.584,
      "step": 1620
    },
    {
      "epoch": 0.23310354630090258,
      "grad_norm": 0.6454874873161316,
      "learning_rate": 0.0002767055144968732,
      "loss": 0.5852,
      "step": 1640
    },
    {
      "epoch": 0.2359462724753038,
      "grad_norm": 0.6666456460952759,
      "learning_rate": 0.00027642126208072767,
      "loss": 0.5828,
      "step": 1660
    },
    {
      "epoch": 0.23878899864970507,
      "grad_norm": 0.6184484958648682,
      "learning_rate": 0.0002761370096645821,
      "loss": 0.5838,
      "step": 1680
    },
    {
      "epoch": 0.24163172482410633,
      "grad_norm": 0.571279764175415,
      "learning_rate": 0.0002758527572484366,
      "loss": 0.5925,
      "step": 1700
    },
    {
      "epoch": 0.24447445099850756,
      "grad_norm": 0.6228147745132446,
      "learning_rate": 0.00027556850483229106,
      "loss": 0.5839,
      "step": 1720
    },
    {
      "epoch": 0.24731717717290883,
      "grad_norm": 0.6236391067504883,
      "learning_rate": 0.00027528425241614554,
      "loss": 0.5883,
      "step": 1740
    },
    {
      "epoch": 0.2501599033473101,
      "grad_norm": 0.636554479598999,
      "learning_rate": 0.00027499999999999996,
      "loss": 0.5931,
      "step": 1760
    },
    {
      "epoch": 0.2530026295217113,
      "grad_norm": 0.6327835321426392,
      "learning_rate": 0.00027471574758385444,
      "loss": 0.6006,
      "step": 1780
    },
    {
      "epoch": 0.25584535569611255,
      "grad_norm": 0.5606619715690613,
      "learning_rate": 0.0002744314951677089,
      "loss": 0.5939,
      "step": 1800
    },
    {
      "epoch": 0.25868808187051384,
      "grad_norm": 0.5819507837295532,
      "learning_rate": 0.00027414724275156335,
      "loss": 0.5949,
      "step": 1820
    },
    {
      "epoch": 0.2615308080449151,
      "grad_norm": 0.6072883009910583,
      "learning_rate": 0.0002738629903354178,
      "loss": 0.5699,
      "step": 1840
    },
    {
      "epoch": 0.2643735342193163,
      "grad_norm": 0.6272257566452026,
      "learning_rate": 0.0002735787379192723,
      "loss": 0.5663,
      "step": 1860
    },
    {
      "epoch": 0.2672162603937176,
      "grad_norm": 0.6063715219497681,
      "learning_rate": 0.00027329448550312673,
      "loss": 0.5965,
      "step": 1880
    },
    {
      "epoch": 0.27005898656811883,
      "grad_norm": 0.6090818643569946,
      "learning_rate": 0.0002730102330869812,
      "loss": 0.5899,
      "step": 1900
    },
    {
      "epoch": 0.27290171274252006,
      "grad_norm": 0.6526395082473755,
      "learning_rate": 0.0002727259806708357,
      "loss": 0.5784,
      "step": 1920
    },
    {
      "epoch": 0.27574443891692135,
      "grad_norm": 0.5974206924438477,
      "learning_rate": 0.0002724417282546901,
      "loss": 0.572,
      "step": 1940
    },
    {
      "epoch": 0.2785871650913226,
      "grad_norm": 0.695592999458313,
      "learning_rate": 0.0002721574758385446,
      "loss": 0.5748,
      "step": 1960
    },
    {
      "epoch": 0.2814298912657238,
      "grad_norm": 0.6379124522209167,
      "learning_rate": 0.0002718732234223991,
      "loss": 0.5798,
      "step": 1980
    },
    {
      "epoch": 0.28427261744012505,
      "grad_norm": 0.5933524370193481,
      "learning_rate": 0.00027158897100625355,
      "loss": 0.5692,
      "step": 2000
    },
    {
      "epoch": 0.28711534361452634,
      "grad_norm": 0.6540465354919434,
      "learning_rate": 0.000271304718590108,
      "loss": 0.5801,
      "step": 2020
    },
    {
      "epoch": 0.2899580697889276,
      "grad_norm": 0.6396123170852661,
      "learning_rate": 0.00027102046617396246,
      "loss": 0.5845,
      "step": 2040
    },
    {
      "epoch": 0.2928007959633288,
      "grad_norm": 0.6306635141372681,
      "learning_rate": 0.00027073621375781694,
      "loss": 0.5766,
      "step": 2060
    },
    {
      "epoch": 0.2956435221377301,
      "grad_norm": 0.5936034917831421,
      "learning_rate": 0.00027045196134167136,
      "loss": 0.561,
      "step": 2080
    },
    {
      "epoch": 0.29848624831213133,
      "grad_norm": 0.5978198647499084,
      "learning_rate": 0.00027016770892552584,
      "loss": 0.577,
      "step": 2100
    },
    {
      "epoch": 0.30132897448653256,
      "grad_norm": 0.6228933334350586,
      "learning_rate": 0.0002698834565093803,
      "loss": 0.577,
      "step": 2120
    },
    {
      "epoch": 0.30417170066093385,
      "grad_norm": 0.6573242545127869,
      "learning_rate": 0.00026959920409323475,
      "loss": 0.5871,
      "step": 2140
    },
    {
      "epoch": 0.3070144268353351,
      "grad_norm": 0.6257950663566589,
      "learning_rate": 0.00026931495167708923,
      "loss": 0.5659,
      "step": 2160
    },
    {
      "epoch": 0.3098571530097363,
      "grad_norm": 0.5932577252388,
      "learning_rate": 0.0002690306992609437,
      "loss": 0.5714,
      "step": 2180
    },
    {
      "epoch": 0.3126998791841376,
      "grad_norm": 0.6260507702827454,
      "learning_rate": 0.00026874644684479813,
      "loss": 0.5598,
      "step": 2200
    },
    {
      "epoch": 0.31554260535853884,
      "grad_norm": 0.5915515422821045,
      "learning_rate": 0.0002684621944286526,
      "loss": 0.5662,
      "step": 2220
    },
    {
      "epoch": 0.3183853315329401,
      "grad_norm": 0.6080064177513123,
      "learning_rate": 0.0002681779420125071,
      "loss": 0.5655,
      "step": 2240
    },
    {
      "epoch": 0.32122805770734136,
      "grad_norm": 0.5788610577583313,
      "learning_rate": 0.00026789368959636157,
      "loss": 0.5626,
      "step": 2260
    },
    {
      "epoch": 0.3240707838817426,
      "grad_norm": 0.6026416420936584,
      "learning_rate": 0.000267609437180216,
      "loss": 0.5651,
      "step": 2280
    },
    {
      "epoch": 0.32691351005614383,
      "grad_norm": 0.5764790773391724,
      "learning_rate": 0.0002673251847640705,
      "loss": 0.5711,
      "step": 2300
    },
    {
      "epoch": 0.3297562362305451,
      "grad_norm": 0.5855320692062378,
      "learning_rate": 0.00026704093234792496,
      "loss": 0.5773,
      "step": 2320
    },
    {
      "epoch": 0.33259896240494635,
      "grad_norm": 0.6146620512008667,
      "learning_rate": 0.0002667566799317794,
      "loss": 0.5725,
      "step": 2340
    },
    {
      "epoch": 0.3354416885793476,
      "grad_norm": 0.589505136013031,
      "learning_rate": 0.00026647242751563386,
      "loss": 0.5624,
      "step": 2360
    },
    {
      "epoch": 0.3382844147537488,
      "grad_norm": 0.6566455364227295,
      "learning_rate": 0.00026618817509948834,
      "loss": 0.567,
      "step": 2380
    },
    {
      "epoch": 0.3411271409281501,
      "grad_norm": 0.6193392276763916,
      "learning_rate": 0.00026590392268334277,
      "loss": 0.5787,
      "step": 2400
    },
    {
      "epoch": 0.34396986710255134,
      "grad_norm": 0.5843732953071594,
      "learning_rate": 0.00026561967026719725,
      "loss": 0.5702,
      "step": 2420
    },
    {
      "epoch": 0.3468125932769526,
      "grad_norm": 0.557308554649353,
      "learning_rate": 0.0002653354178510517,
      "loss": 0.566,
      "step": 2440
    },
    {
      "epoch": 0.34965531945135386,
      "grad_norm": 0.5795814394950867,
      "learning_rate": 0.00026505116543490615,
      "loss": 0.5636,
      "step": 2460
    },
    {
      "epoch": 0.3524980456257551,
      "grad_norm": 0.6707448959350586,
      "learning_rate": 0.00026476691301876063,
      "loss": 0.5635,
      "step": 2480
    },
    {
      "epoch": 0.35534077180015633,
      "grad_norm": 0.6462289690971375,
      "learning_rate": 0.0002644826606026151,
      "loss": 0.5661,
      "step": 2500
    },
    {
      "epoch": 0.3581834979745576,
      "grad_norm": 0.5905725359916687,
      "learning_rate": 0.0002641984081864696,
      "loss": 0.5438,
      "step": 2520
    },
    {
      "epoch": 0.36102622414895885,
      "grad_norm": 0.5959571599960327,
      "learning_rate": 0.000263914155770324,
      "loss": 0.5644,
      "step": 2540
    },
    {
      "epoch": 0.3638689503233601,
      "grad_norm": 0.5954847931861877,
      "learning_rate": 0.0002636299033541785,
      "loss": 0.5539,
      "step": 2560
    },
    {
      "epoch": 0.3667116764977614,
      "grad_norm": 0.6003422737121582,
      "learning_rate": 0.000263345650938033,
      "loss": 0.5502,
      "step": 2580
    },
    {
      "epoch": 0.3695544026721626,
      "grad_norm": 0.5738318562507629,
      "learning_rate": 0.0002630613985218874,
      "loss": 0.5775,
      "step": 2600
    },
    {
      "epoch": 0.37239712884656384,
      "grad_norm": 0.591930627822876,
      "learning_rate": 0.0002627771461057419,
      "loss": 0.562,
      "step": 2620
    },
    {
      "epoch": 0.37523985502096513,
      "grad_norm": 0.5740834474563599,
      "learning_rate": 0.00026249289368959636,
      "loss": 0.5619,
      "step": 2640
    },
    {
      "epoch": 0.37808258119536636,
      "grad_norm": 0.5793691873550415,
      "learning_rate": 0.0002622086412734508,
      "loss": 0.5636,
      "step": 2660
    },
    {
      "epoch": 0.3809253073697676,
      "grad_norm": 0.5528129935264587,
      "learning_rate": 0.00026192438885730526,
      "loss": 0.5491,
      "step": 2680
    },
    {
      "epoch": 0.3837680335441689,
      "grad_norm": 0.6539316773414612,
      "learning_rate": 0.00026164013644115974,
      "loss": 0.5644,
      "step": 2700
    },
    {
      "epoch": 0.3866107597185701,
      "grad_norm": 0.6392313838005066,
      "learning_rate": 0.00026135588402501417,
      "loss": 0.585,
      "step": 2720
    },
    {
      "epoch": 0.38945348589297135,
      "grad_norm": 0.6737197041511536,
      "learning_rate": 0.00026107163160886865,
      "loss": 0.5602,
      "step": 2740
    },
    {
      "epoch": 0.3922962120673726,
      "grad_norm": 0.5418497323989868,
      "learning_rate": 0.0002607873791927231,
      "loss": 0.5515,
      "step": 2760
    },
    {
      "epoch": 0.3951389382417739,
      "grad_norm": 0.6168406009674072,
      "learning_rate": 0.0002605031267765776,
      "loss": 0.5545,
      "step": 2780
    },
    {
      "epoch": 0.3979816644161751,
      "grad_norm": 0.5855759978294373,
      "learning_rate": 0.00026021887436043203,
      "loss": 0.5638,
      "step": 2800
    },
    {
      "epoch": 0.40082439059057634,
      "grad_norm": 0.6111850142478943,
      "learning_rate": 0.0002599346219442865,
      "loss": 0.5629,
      "step": 2820
    },
    {
      "epoch": 0.40366711676497763,
      "grad_norm": 0.584621250629425,
      "learning_rate": 0.000259650369528141,
      "loss": 0.5488,
      "step": 2840
    },
    {
      "epoch": 0.40650984293937886,
      "grad_norm": 0.599449098110199,
      "learning_rate": 0.0002593661171119954,
      "loss": 0.553,
      "step": 2860
    },
    {
      "epoch": 0.4093525691137801,
      "grad_norm": 0.6236058473587036,
      "learning_rate": 0.0002590818646958499,
      "loss": 0.5789,
      "step": 2880
    },
    {
      "epoch": 0.4121952952881814,
      "grad_norm": 0.6010106801986694,
      "learning_rate": 0.0002587976122797044,
      "loss": 0.569,
      "step": 2900
    },
    {
      "epoch": 0.4150380214625826,
      "grad_norm": 0.5756456255912781,
      "learning_rate": 0.0002585133598635588,
      "loss": 0.5626,
      "step": 2920
    },
    {
      "epoch": 0.41788074763698385,
      "grad_norm": 0.5850918292999268,
      "learning_rate": 0.0002582291074474133,
      "loss": 0.5575,
      "step": 2940
    },
    {
      "epoch": 0.42072347381138514,
      "grad_norm": 0.5956099629402161,
      "learning_rate": 0.0002579448550312677,
      "loss": 0.5494,
      "step": 2960
    },
    {
      "epoch": 0.4235661999857864,
      "grad_norm": 0.5696232914924622,
      "learning_rate": 0.0002576606026151222,
      "loss": 0.5475,
      "step": 2980
    },
    {
      "epoch": 0.4264089261601876,
      "grad_norm": 0.5602251291275024,
      "learning_rate": 0.00025737635019897667,
      "loss": 0.5637,
      "step": 3000
    },
    {
      "epoch": 0.4292516523345889,
      "grad_norm": 0.5781689882278442,
      "learning_rate": 0.0002570920977828311,
      "loss": 0.5588,
      "step": 3020
    },
    {
      "epoch": 0.43209437850899013,
      "grad_norm": 0.6103278398513794,
      "learning_rate": 0.0002568078453666856,
      "loss": 0.5545,
      "step": 3040
    },
    {
      "epoch": 0.43493710468339136,
      "grad_norm": 0.6229214072227478,
      "learning_rate": 0.00025652359295054005,
      "loss": 0.554,
      "step": 3060
    },
    {
      "epoch": 0.4377798308577926,
      "grad_norm": 0.6158135533332825,
      "learning_rate": 0.00025623934053439453,
      "loss": 0.5495,
      "step": 3080
    },
    {
      "epoch": 0.4406225570321939,
      "grad_norm": 0.5559866428375244,
      "learning_rate": 0.000255955088118249,
      "loss": 0.5444,
      "step": 3100
    },
    {
      "epoch": 0.4434652832065951,
      "grad_norm": 0.606304407119751,
      "learning_rate": 0.00025567083570210344,
      "loss": 0.5764,
      "step": 3120
    },
    {
      "epoch": 0.44630800938099635,
      "grad_norm": 0.6201961040496826,
      "learning_rate": 0.0002553865832859579,
      "loss": 0.5435,
      "step": 3140
    },
    {
      "epoch": 0.44915073555539764,
      "grad_norm": 0.649878740310669,
      "learning_rate": 0.00025510233086981234,
      "loss": 0.5426,
      "step": 3160
    },
    {
      "epoch": 0.4519934617297989,
      "grad_norm": 0.5459410548210144,
      "learning_rate": 0.0002548180784536668,
      "loss": 0.5423,
      "step": 3180
    },
    {
      "epoch": 0.4548361879042001,
      "grad_norm": 0.5996789932250977,
      "learning_rate": 0.0002545338260375213,
      "loss": 0.5669,
      "step": 3200
    },
    {
      "epoch": 0.4576789140786014,
      "grad_norm": 0.5503170490264893,
      "learning_rate": 0.0002542495736213757,
      "loss": 0.5555,
      "step": 3220
    },
    {
      "epoch": 0.46052164025300263,
      "grad_norm": 0.5926568508148193,
      "learning_rate": 0.00025396532120523026,
      "loss": 0.5368,
      "step": 3240
    },
    {
      "epoch": 0.46336436642740386,
      "grad_norm": 0.5650294423103333,
      "learning_rate": 0.0002536810687890847,
      "loss": 0.5363,
      "step": 3260
    },
    {
      "epoch": 0.46620709260180515,
      "grad_norm": 0.6078860759735107,
      "learning_rate": 0.00025339681637293916,
      "loss": 0.5552,
      "step": 3280
    },
    {
      "epoch": 0.4690498187762064,
      "grad_norm": 0.5284907221794128,
      "learning_rate": 0.00025311256395679364,
      "loss": 0.5408,
      "step": 3300
    },
    {
      "epoch": 0.4718925449506076,
      "grad_norm": 0.5767223238945007,
      "learning_rate": 0.00025282831154064807,
      "loss": 0.5662,
      "step": 3320
    },
    {
      "epoch": 0.4747352711250089,
      "grad_norm": 0.5696093440055847,
      "learning_rate": 0.00025254405912450255,
      "loss": 0.5525,
      "step": 3340
    },
    {
      "epoch": 0.47757799729941014,
      "grad_norm": 0.5593514442443848,
      "learning_rate": 0.00025225980670835703,
      "loss": 0.5324,
      "step": 3360
    },
    {
      "epoch": 0.4804207234738114,
      "grad_norm": 0.6016511917114258,
      "learning_rate": 0.00025197555429221145,
      "loss": 0.5463,
      "step": 3380
    },
    {
      "epoch": 0.48326344964821266,
      "grad_norm": 0.5671796202659607,
      "learning_rate": 0.00025169130187606593,
      "loss": 0.5331,
      "step": 3400
    },
    {
      "epoch": 0.4861061758226139,
      "grad_norm": 0.5720767378807068,
      "learning_rate": 0.00025140704945992036,
      "loss": 0.5474,
      "step": 3420
    },
    {
      "epoch": 0.48894890199701513,
      "grad_norm": 0.6258890628814697,
      "learning_rate": 0.00025112279704377484,
      "loss": 0.5322,
      "step": 3440
    },
    {
      "epoch": 0.49179162817141636,
      "grad_norm": 0.5782374739646912,
      "learning_rate": 0.0002508385446276293,
      "loss": 0.5526,
      "step": 3460
    },
    {
      "epoch": 0.49463435434581765,
      "grad_norm": 0.6320413947105408,
      "learning_rate": 0.00025055429221148374,
      "loss": 0.5543,
      "step": 3480
    },
    {
      "epoch": 0.4974770805202189,
      "grad_norm": 0.6435434818267822,
      "learning_rate": 0.0002502700397953383,
      "loss": 0.5561,
      "step": 3500
    },
    {
      "epoch": 0.5003198066946202,
      "grad_norm": 0.6074697971343994,
      "learning_rate": 0.0002499857873791927,
      "loss": 0.5451,
      "step": 3520
    },
    {
      "epoch": 0.5031625328690214,
      "grad_norm": 0.5393642783164978,
      "learning_rate": 0.0002497015349630472,
      "loss": 0.555,
      "step": 3540
    },
    {
      "epoch": 0.5060052590434226,
      "grad_norm": 0.6175057888031006,
      "learning_rate": 0.00024941728254690166,
      "loss": 0.5514,
      "step": 3560
    },
    {
      "epoch": 0.5088479852178239,
      "grad_norm": 0.5738746523857117,
      "learning_rate": 0.0002491330301307561,
      "loss": 0.5477,
      "step": 3580
    },
    {
      "epoch": 0.5116907113922251,
      "grad_norm": 0.595355749130249,
      "learning_rate": 0.00024884877771461057,
      "loss": 0.5441,
      "step": 3600
    },
    {
      "epoch": 0.5145334375666264,
      "grad_norm": 0.5887110233306885,
      "learning_rate": 0.000248564525298465,
      "loss": 0.5434,
      "step": 3620
    },
    {
      "epoch": 0.5173761637410277,
      "grad_norm": 0.6044406890869141,
      "learning_rate": 0.00024828027288231947,
      "loss": 0.5556,
      "step": 3640
    },
    {
      "epoch": 0.5202188899154289,
      "grad_norm": 0.5932636260986328,
      "learning_rate": 0.00024799602046617395,
      "loss": 0.548,
      "step": 3660
    },
    {
      "epoch": 0.5230616160898302,
      "grad_norm": 0.5757077932357788,
      "learning_rate": 0.0002477117680500284,
      "loss": 0.5548,
      "step": 3680
    },
    {
      "epoch": 0.5259043422642314,
      "grad_norm": 0.5719469785690308,
      "learning_rate": 0.00024742751563388286,
      "loss": 0.5493,
      "step": 3700
    },
    {
      "epoch": 0.5287470684386326,
      "grad_norm": 0.5564594864845276,
      "learning_rate": 0.00024714326321773734,
      "loss": 0.5445,
      "step": 3720
    },
    {
      "epoch": 0.5315897946130339,
      "grad_norm": 0.6540044546127319,
      "learning_rate": 0.00024685901080159176,
      "loss": 0.5615,
      "step": 3740
    },
    {
      "epoch": 0.5344325207874352,
      "grad_norm": 0.621612548828125,
      "learning_rate": 0.0002465747583854463,
      "loss": 0.5587,
      "step": 3760
    },
    {
      "epoch": 0.5372752469618364,
      "grad_norm": 0.6125980019569397,
      "learning_rate": 0.0002462905059693007,
      "loss": 0.5539,
      "step": 3780
    },
    {
      "epoch": 0.5401179731362377,
      "grad_norm": 0.6060158610343933,
      "learning_rate": 0.0002460062535531552,
      "loss": 0.5398,
      "step": 3800
    },
    {
      "epoch": 0.542960699310639,
      "grad_norm": 0.5901080369949341,
      "learning_rate": 0.0002457220011370096,
      "loss": 0.5502,
      "step": 3820
    },
    {
      "epoch": 0.5458034254850401,
      "grad_norm": 0.6128995418548584,
      "learning_rate": 0.0002454377487208641,
      "loss": 0.5534,
      "step": 3840
    },
    {
      "epoch": 0.5486461516594414,
      "grad_norm": 0.5850970149040222,
      "learning_rate": 0.0002451534963047186,
      "loss": 0.5421,
      "step": 3860
    },
    {
      "epoch": 0.5514888778338427,
      "grad_norm": 0.5679976940155029,
      "learning_rate": 0.000244869243888573,
      "loss": 0.5172,
      "step": 3880
    },
    {
      "epoch": 0.5543316040082439,
      "grad_norm": 0.6045100092887878,
      "learning_rate": 0.0002445849914724275,
      "loss": 0.5417,
      "step": 3900
    },
    {
      "epoch": 0.5571743301826452,
      "grad_norm": 0.599398672580719,
      "learning_rate": 0.00024430073905628197,
      "loss": 0.5396,
      "step": 3920
    },
    {
      "epoch": 0.5600170563570465,
      "grad_norm": 0.6026158928871155,
      "learning_rate": 0.00024401648664013642,
      "loss": 0.5345,
      "step": 3940
    },
    {
      "epoch": 0.5628597825314476,
      "grad_norm": 0.5961952805519104,
      "learning_rate": 0.0002437322342239909,
      "loss": 0.5444,
      "step": 3960
    },
    {
      "epoch": 0.5657025087058489,
      "grad_norm": 0.5848454833030701,
      "learning_rate": 0.00024344798180784535,
      "loss": 0.5417,
      "step": 3980
    },
    {
      "epoch": 0.5685452348802501,
      "grad_norm": 0.5589332580566406,
      "learning_rate": 0.0002431637293916998,
      "loss": 0.5393,
      "step": 4000
    },
    {
      "epoch": 0.5713879610546514,
      "grad_norm": 0.6038827896118164,
      "learning_rate": 0.00024287947697555426,
      "loss": 0.5385,
      "step": 4020
    },
    {
      "epoch": 0.5742306872290527,
      "grad_norm": 0.5429565906524658,
      "learning_rate": 0.00024259522455940874,
      "loss": 0.5463,
      "step": 4040
    },
    {
      "epoch": 0.5770734134034539,
      "grad_norm": 0.5592791438102722,
      "learning_rate": 0.0002423109721432632,
      "loss": 0.5385,
      "step": 4060
    },
    {
      "epoch": 0.5799161395778551,
      "grad_norm": 0.5910287499427795,
      "learning_rate": 0.00024202671972711764,
      "loss": 0.5561,
      "step": 4080
    },
    {
      "epoch": 0.5827588657522564,
      "grad_norm": 0.5863296389579773,
      "learning_rate": 0.00024174246731097212,
      "loss": 0.5513,
      "step": 4100
    },
    {
      "epoch": 0.5856015919266576,
      "grad_norm": 0.5677945613861084,
      "learning_rate": 0.00024145821489482657,
      "loss": 0.5382,
      "step": 4120
    },
    {
      "epoch": 0.5884443181010589,
      "grad_norm": 0.5647968053817749,
      "learning_rate": 0.00024117396247868103,
      "loss": 0.5378,
      "step": 4140
    },
    {
      "epoch": 0.5912870442754602,
      "grad_norm": 0.5805327296257019,
      "learning_rate": 0.00024088971006253553,
      "loss": 0.536,
      "step": 4160
    },
    {
      "epoch": 0.5941297704498614,
      "grad_norm": 0.6479085683822632,
      "learning_rate": 0.00024060545764638999,
      "loss": 0.5421,
      "step": 4180
    },
    {
      "epoch": 0.5969724966242627,
      "grad_norm": 0.5601728558540344,
      "learning_rate": 0.00024032120523024444,
      "loss": 0.5426,
      "step": 4200
    },
    {
      "epoch": 0.599815222798664,
      "grad_norm": 0.6128291487693787,
      "learning_rate": 0.00024003695281409892,
      "loss": 0.5434,
      "step": 4220
    },
    {
      "epoch": 0.6026579489730651,
      "grad_norm": 0.6319482326507568,
      "learning_rate": 0.00023975270039795337,
      "loss": 0.5339,
      "step": 4240
    },
    {
      "epoch": 0.6055006751474664,
      "grad_norm": 0.5599532723426819,
      "learning_rate": 0.00023946844798180782,
      "loss": 0.5563,
      "step": 4260
    },
    {
      "epoch": 0.6083434013218677,
      "grad_norm": 0.5805384516716003,
      "learning_rate": 0.00023918419556566228,
      "loss": 0.5389,
      "step": 4280
    },
    {
      "epoch": 0.6111861274962689,
      "grad_norm": 0.5822192430496216,
      "learning_rate": 0.00023889994314951676,
      "loss": 0.5368,
      "step": 4300
    },
    {
      "epoch": 0.6140288536706702,
      "grad_norm": 0.5872097611427307,
      "learning_rate": 0.0002386156907333712,
      "loss": 0.5436,
      "step": 4320
    },
    {
      "epoch": 0.6168715798450715,
      "grad_norm": 0.6051740646362305,
      "learning_rate": 0.00023833143831722566,
      "loss": 0.5271,
      "step": 4340
    },
    {
      "epoch": 0.6197143060194726,
      "grad_norm": 0.5606023073196411,
      "learning_rate": 0.00023804718590108014,
      "loss": 0.5338,
      "step": 4360
    },
    {
      "epoch": 0.6225570321938739,
      "grad_norm": 0.5858314037322998,
      "learning_rate": 0.0002377629334849346,
      "loss": 0.5381,
      "step": 4380
    },
    {
      "epoch": 0.6253997583682752,
      "grad_norm": 0.615108072757721,
      "learning_rate": 0.00023747868106878904,
      "loss": 0.5136,
      "step": 4400
    },
    {
      "epoch": 0.6282424845426764,
      "grad_norm": 0.6123197078704834,
      "learning_rate": 0.00023719442865264355,
      "loss": 0.546,
      "step": 4420
    },
    {
      "epoch": 0.6310852107170777,
      "grad_norm": 0.5697813034057617,
      "learning_rate": 0.000236910176236498,
      "loss": 0.5489,
      "step": 4440
    },
    {
      "epoch": 0.633927936891479,
      "grad_norm": 0.6098710894584656,
      "learning_rate": 0.00023662592382035246,
      "loss": 0.5334,
      "step": 4460
    },
    {
      "epoch": 0.6367706630658801,
      "grad_norm": 0.583770751953125,
      "learning_rate": 0.0002363416714042069,
      "loss": 0.5303,
      "step": 4480
    },
    {
      "epoch": 0.6396133892402814,
      "grad_norm": 0.5855697989463806,
      "learning_rate": 0.0002360574189880614,
      "loss": 0.5458,
      "step": 4500
    },
    {
      "epoch": 0.6424561154146827,
      "grad_norm": 0.6310400366783142,
      "learning_rate": 0.00023577316657191584,
      "loss": 0.536,
      "step": 4520
    },
    {
      "epoch": 0.6452988415890839,
      "grad_norm": 0.5955621004104614,
      "learning_rate": 0.0002354889141557703,
      "loss": 0.5431,
      "step": 4540
    },
    {
      "epoch": 0.6481415677634852,
      "grad_norm": 0.5351962447166443,
      "learning_rate": 0.00023520466173962477,
      "loss": 0.5264,
      "step": 4560
    },
    {
      "epoch": 0.6509842939378865,
      "grad_norm": 0.5888294577598572,
      "learning_rate": 0.00023492040932347923,
      "loss": 0.5405,
      "step": 4580
    },
    {
      "epoch": 0.6538270201122877,
      "grad_norm": 0.5734386444091797,
      "learning_rate": 0.00023463615690733368,
      "loss": 0.5329,
      "step": 4600
    },
    {
      "epoch": 0.656669746286689,
      "grad_norm": 0.5929276943206787,
      "learning_rate": 0.00023435190449118816,
      "loss": 0.5104,
      "step": 4620
    },
    {
      "epoch": 0.6595124724610902,
      "grad_norm": 0.6014058589935303,
      "learning_rate": 0.0002340676520750426,
      "loss": 0.5294,
      "step": 4640
    },
    {
      "epoch": 0.6623551986354914,
      "grad_norm": 0.5893369317054749,
      "learning_rate": 0.00023378339965889706,
      "loss": 0.5256,
      "step": 4660
    },
    {
      "epoch": 0.6651979248098927,
      "grad_norm": 0.6308799982070923,
      "learning_rate": 0.00023349914724275152,
      "loss": 0.5275,
      "step": 4680
    },
    {
      "epoch": 0.668040650984294,
      "grad_norm": 0.5621747374534607,
      "learning_rate": 0.00023321489482660602,
      "loss": 0.522,
      "step": 4700
    },
    {
      "epoch": 0.6708833771586952,
      "grad_norm": 0.5920476913452148,
      "learning_rate": 0.00023293064241046047,
      "loss": 0.5481,
      "step": 4720
    },
    {
      "epoch": 0.6737261033330965,
      "grad_norm": 0.5866132974624634,
      "learning_rate": 0.00023264638999431493,
      "loss": 0.5255,
      "step": 4740
    },
    {
      "epoch": 0.6765688295074976,
      "grad_norm": 0.6142862439155579,
      "learning_rate": 0.0002323621375781694,
      "loss": 0.5327,
      "step": 4760
    },
    {
      "epoch": 0.6794115556818989,
      "grad_norm": 0.5576959252357483,
      "learning_rate": 0.00023207788516202386,
      "loss": 0.5312,
      "step": 4780
    },
    {
      "epoch": 0.6822542818563002,
      "grad_norm": 0.5622161626815796,
      "learning_rate": 0.0002317936327458783,
      "loss": 0.5243,
      "step": 4800
    },
    {
      "epoch": 0.6850970080307014,
      "grad_norm": 0.5563510060310364,
      "learning_rate": 0.0002315093803297328,
      "loss": 0.5293,
      "step": 4820
    },
    {
      "epoch": 0.6879397342051027,
      "grad_norm": 0.5838661193847656,
      "learning_rate": 0.00023122512791358724,
      "loss": 0.5276,
      "step": 4840
    },
    {
      "epoch": 0.690782460379504,
      "grad_norm": 0.5936771035194397,
      "learning_rate": 0.0002309408754974417,
      "loss": 0.5305,
      "step": 4860
    },
    {
      "epoch": 0.6936251865539051,
      "grad_norm": 0.5991916060447693,
      "learning_rate": 0.00023065662308129615,
      "loss": 0.5288,
      "step": 4880
    },
    {
      "epoch": 0.6964679127283064,
      "grad_norm": 0.5476929545402527,
      "learning_rate": 0.00023037237066515063,
      "loss": 0.5314,
      "step": 4900
    },
    {
      "epoch": 0.6993106389027077,
      "grad_norm": 0.586914598941803,
      "learning_rate": 0.00023008811824900508,
      "loss": 0.5391,
      "step": 4920
    },
    {
      "epoch": 0.7021533650771089,
      "grad_norm": 0.5990623235702515,
      "learning_rate": 0.00022980386583285953,
      "loss": 0.5379,
      "step": 4940
    },
    {
      "epoch": 0.7049960912515102,
      "grad_norm": 0.5856022238731384,
      "learning_rate": 0.00022951961341671404,
      "loss": 0.529,
      "step": 4960
    },
    {
      "epoch": 0.7078388174259115,
      "grad_norm": 0.610438346862793,
      "learning_rate": 0.0002292353610005685,
      "loss": 0.5305,
      "step": 4980
    },
    {
      "epoch": 0.7106815436003127,
      "grad_norm": 0.6043747663497925,
      "learning_rate": 0.00022895110858442294,
      "loss": 0.5439,
      "step": 5000
    },
    {
      "epoch": 0.713524269774714,
      "grad_norm": 0.580646812915802,
      "learning_rate": 0.00022866685616827742,
      "loss": 0.5333,
      "step": 5020
    },
    {
      "epoch": 0.7163669959491152,
      "grad_norm": 0.5493490695953369,
      "learning_rate": 0.00022838260375213188,
      "loss": 0.5468,
      "step": 5040
    },
    {
      "epoch": 0.7192097221235164,
      "grad_norm": 0.6054022312164307,
      "learning_rate": 0.00022809835133598633,
      "loss": 0.5229,
      "step": 5060
    },
    {
      "epoch": 0.7220524482979177,
      "grad_norm": 0.608187735080719,
      "learning_rate": 0.0002278140989198408,
      "loss": 0.5205,
      "step": 5080
    },
    {
      "epoch": 0.724895174472319,
      "grad_norm": 0.6151040196418762,
      "learning_rate": 0.00022752984650369526,
      "loss": 0.5201,
      "step": 5100
    },
    {
      "epoch": 0.7277379006467202,
      "grad_norm": 0.5480471849441528,
      "learning_rate": 0.0002272455940875497,
      "loss": 0.5173,
      "step": 5120
    },
    {
      "epoch": 0.7305806268211215,
      "grad_norm": 0.5877110958099365,
      "learning_rate": 0.00022696134167140417,
      "loss": 0.5238,
      "step": 5140
    },
    {
      "epoch": 0.7334233529955227,
      "grad_norm": 0.5545012950897217,
      "learning_rate": 0.00022667708925525867,
      "loss": 0.5255,
      "step": 5160
    },
    {
      "epoch": 0.7362660791699239,
      "grad_norm": 0.5367186665534973,
      "learning_rate": 0.00022639283683911313,
      "loss": 0.5203,
      "step": 5180
    },
    {
      "epoch": 0.7391088053443252,
      "grad_norm": 0.5932608246803284,
      "learning_rate": 0.00022610858442296758,
      "loss": 0.5166,
      "step": 5200
    },
    {
      "epoch": 0.7419515315187265,
      "grad_norm": 0.5667805671691895,
      "learning_rate": 0.00022582433200682206,
      "loss": 0.5352,
      "step": 5220
    },
    {
      "epoch": 0.7447942576931277,
      "grad_norm": 0.6814128160476685,
      "learning_rate": 0.0002255400795906765,
      "loss": 0.5264,
      "step": 5240
    },
    {
      "epoch": 0.747636983867529,
      "grad_norm": 0.5922266840934753,
      "learning_rate": 0.00022525582717453096,
      "loss": 0.5303,
      "step": 5260
    },
    {
      "epoch": 0.7504797100419303,
      "grad_norm": 0.6281228065490723,
      "learning_rate": 0.00022497157475838544,
      "loss": 0.5221,
      "step": 5280
    },
    {
      "epoch": 0.7533224362163314,
      "grad_norm": 0.6182637810707092,
      "learning_rate": 0.0002246873223422399,
      "loss": 0.5379,
      "step": 5300
    },
    {
      "epoch": 0.7561651623907327,
      "grad_norm": 0.5999391674995422,
      "learning_rate": 0.00022440306992609435,
      "loss": 0.5235,
      "step": 5320
    },
    {
      "epoch": 0.759007888565134,
      "grad_norm": 0.643182098865509,
      "learning_rate": 0.0002241188175099488,
      "loss": 0.5258,
      "step": 5340
    },
    {
      "epoch": 0.7618506147395352,
      "grad_norm": 0.5861769914627075,
      "learning_rate": 0.00022383456509380328,
      "loss": 0.538,
      "step": 5360
    },
    {
      "epoch": 0.7646933409139365,
      "grad_norm": 0.5835195779800415,
      "learning_rate": 0.00022355031267765773,
      "loss": 0.5383,
      "step": 5380
    },
    {
      "epoch": 0.7675360670883378,
      "grad_norm": 0.6889169812202454,
      "learning_rate": 0.00022326606026151218,
      "loss": 0.5272,
      "step": 5400
    },
    {
      "epoch": 0.770378793262739,
      "grad_norm": 0.6191014051437378,
      "learning_rate": 0.0002229818078453667,
      "loss": 0.5409,
      "step": 5420
    },
    {
      "epoch": 0.7732215194371402,
      "grad_norm": 0.5859341025352478,
      "learning_rate": 0.00022269755542922114,
      "loss": 0.5354,
      "step": 5440
    },
    {
      "epoch": 0.7760642456115414,
      "grad_norm": 0.5931044816970825,
      "learning_rate": 0.0002224133030130756,
      "loss": 0.5161,
      "step": 5460
    },
    {
      "epoch": 0.7789069717859427,
      "grad_norm": 0.5754153728485107,
      "learning_rate": 0.00022212905059693008,
      "loss": 0.5129,
      "step": 5480
    },
    {
      "epoch": 0.781749697960344,
      "grad_norm": 0.5786070227622986,
      "learning_rate": 0.00022184479818078453,
      "loss": 0.5348,
      "step": 5500
    },
    {
      "epoch": 0.7845924241347452,
      "grad_norm": 0.5459280610084534,
      "learning_rate": 0.00022156054576463898,
      "loss": 0.5144,
      "step": 5520
    },
    {
      "epoch": 0.7874351503091465,
      "grad_norm": 0.6289880275726318,
      "learning_rate": 0.00022127629334849343,
      "loss": 0.5243,
      "step": 5540
    },
    {
      "epoch": 0.7902778764835477,
      "grad_norm": 0.5944435596466064,
      "learning_rate": 0.0002209920409323479,
      "loss": 0.5333,
      "step": 5560
    },
    {
      "epoch": 0.7931206026579489,
      "grad_norm": 0.5969504117965698,
      "learning_rate": 0.00022070778851620236,
      "loss": 0.521,
      "step": 5580
    },
    {
      "epoch": 0.7959633288323502,
      "grad_norm": 0.6091457009315491,
      "learning_rate": 0.00022042353610005682,
      "loss": 0.5266,
      "step": 5600
    },
    {
      "epoch": 0.7988060550067515,
      "grad_norm": 0.6399326920509338,
      "learning_rate": 0.0002201392836839113,
      "loss": 0.5291,
      "step": 5620
    },
    {
      "epoch": 0.8016487811811527,
      "grad_norm": 0.541934609413147,
      "learning_rate": 0.00021985503126776575,
      "loss": 0.5182,
      "step": 5640
    },
    {
      "epoch": 0.804491507355554,
      "grad_norm": 0.6224886178970337,
      "learning_rate": 0.0002195707788516202,
      "loss": 0.5198,
      "step": 5660
    },
    {
      "epoch": 0.8073342335299553,
      "grad_norm": 0.6241923570632935,
      "learning_rate": 0.0002192865264354747,
      "loss": 0.539,
      "step": 5680
    },
    {
      "epoch": 0.8101769597043564,
      "grad_norm": 0.5598939061164856,
      "learning_rate": 0.00021900227401932916,
      "loss": 0.5316,
      "step": 5700
    },
    {
      "epoch": 0.8130196858787577,
      "grad_norm": 0.5888462066650391,
      "learning_rate": 0.0002187180216031836,
      "loss": 0.5405,
      "step": 5720
    },
    {
      "epoch": 0.815862412053159,
      "grad_norm": 0.5938101410865784,
      "learning_rate": 0.00021843376918703807,
      "loss": 0.523,
      "step": 5740
    },
    {
      "epoch": 0.8187051382275602,
      "grad_norm": 0.5902329683303833,
      "learning_rate": 0.00021814951677089255,
      "loss": 0.5245,
      "step": 5760
    },
    {
      "epoch": 0.8215478644019615,
      "grad_norm": 0.5381979942321777,
      "learning_rate": 0.000217865264354747,
      "loss": 0.5037,
      "step": 5780
    },
    {
      "epoch": 0.8243905905763628,
      "grad_norm": 0.5926721692085266,
      "learning_rate": 0.00021758101193860145,
      "loss": 0.526,
      "step": 5800
    },
    {
      "epoch": 0.827233316750764,
      "grad_norm": 0.6142851114273071,
      "learning_rate": 0.00021729675952245593,
      "loss": 0.5209,
      "step": 5820
    },
    {
      "epoch": 0.8300760429251652,
      "grad_norm": 0.5630929470062256,
      "learning_rate": 0.00021701250710631038,
      "loss": 0.5413,
      "step": 5840
    },
    {
      "epoch": 0.8329187690995665,
      "grad_norm": 0.5778456330299377,
      "learning_rate": 0.00021672825469016483,
      "loss": 0.529,
      "step": 5860
    },
    {
      "epoch": 0.8357614952739677,
      "grad_norm": 0.5915812253952026,
      "learning_rate": 0.00021644400227401931,
      "loss": 0.508,
      "step": 5880
    },
    {
      "epoch": 0.838604221448369,
      "grad_norm": 0.5766565799713135,
      "learning_rate": 0.00021615974985787377,
      "loss": 0.5282,
      "step": 5900
    },
    {
      "epoch": 0.8414469476227703,
      "grad_norm": 0.5351023077964783,
      "learning_rate": 0.00021587549744172822,
      "loss": 0.5438,
      "step": 5920
    },
    {
      "epoch": 0.8442896737971715,
      "grad_norm": 0.6005846261978149,
      "learning_rate": 0.00021559124502558273,
      "loss": 0.5297,
      "step": 5940
    },
    {
      "epoch": 0.8471323999715727,
      "grad_norm": 0.613641619682312,
      "learning_rate": 0.00021530699260943718,
      "loss": 0.5247,
      "step": 5960
    },
    {
      "epoch": 0.849975126145974,
      "grad_norm": 0.5912553668022156,
      "learning_rate": 0.00021502274019329163,
      "loss": 0.5258,
      "step": 5980
    },
    {
      "epoch": 0.8528178523203752,
      "grad_norm": 0.577869176864624,
      "learning_rate": 0.00021473848777714608,
      "loss": 0.5461,
      "step": 6000
    },
    {
      "epoch": 0.8556605784947765,
      "grad_norm": 0.5429055690765381,
      "learning_rate": 0.00021445423536100056,
      "loss": 0.5233,
      "step": 6020
    },
    {
      "epoch": 0.8585033046691778,
      "grad_norm": 0.5986088514328003,
      "learning_rate": 0.0002141841955656623,
      "loss": 0.5218,
      "step": 6040
    },
    {
      "epoch": 0.861346030843579,
      "grad_norm": 0.5761025547981262,
      "learning_rate": 0.00021389994314951674,
      "loss": 0.5259,
      "step": 6060
    },
    {
      "epoch": 0.8641887570179803,
      "grad_norm": 0.6086742877960205,
      "learning_rate": 0.0002136156907333712,
      "loss": 0.5153,
      "step": 6080
    },
    {
      "epoch": 0.8670314831923815,
      "grad_norm": 0.5600404739379883,
      "learning_rate": 0.00021333143831722568,
      "loss": 0.519,
      "step": 6100
    },
    {
      "epoch": 0.8698742093667827,
      "grad_norm": 0.6067619323730469,
      "learning_rate": 0.00021304718590108013,
      "loss": 0.5181,
      "step": 6120
    },
    {
      "epoch": 0.872716935541184,
      "grad_norm": 0.5870951414108276,
      "learning_rate": 0.0002127629334849346,
      "loss": 0.5178,
      "step": 6140
    },
    {
      "epoch": 0.8755596617155852,
      "grad_norm": 0.6026812195777893,
      "learning_rate": 0.0002124786810687891,
      "loss": 0.5205,
      "step": 6160
    },
    {
      "epoch": 0.8784023878899865,
      "grad_norm": 0.5518092513084412,
      "learning_rate": 0.00021219442865264354,
      "loss": 0.5296,
      "step": 6180
    },
    {
      "epoch": 0.8812451140643878,
      "grad_norm": 0.5604989528656006,
      "learning_rate": 0.000211910176236498,
      "loss": 0.5252,
      "step": 6200
    },
    {
      "epoch": 0.884087840238789,
      "grad_norm": 0.5999457836151123,
      "learning_rate": 0.00021162592382035245,
      "loss": 0.5298,
      "step": 6220
    },
    {
      "epoch": 0.8869305664131902,
      "grad_norm": 0.6020918488502502,
      "learning_rate": 0.00021134167140420692,
      "loss": 0.5304,
      "step": 6240
    },
    {
      "epoch": 0.8897732925875915,
      "grad_norm": 0.586276650428772,
      "learning_rate": 0.00021105741898806138,
      "loss": 0.5073,
      "step": 6260
    },
    {
      "epoch": 0.8926160187619927,
      "grad_norm": 0.5976669192314148,
      "learning_rate": 0.00021077316657191583,
      "loss": 0.5284,
      "step": 6280
    },
    {
      "epoch": 0.895458744936394,
      "grad_norm": 0.5979356169700623,
      "learning_rate": 0.0002104889141557703,
      "loss": 0.5307,
      "step": 6300
    },
    {
      "epoch": 0.8983014711107953,
      "grad_norm": 0.6125791668891907,
      "learning_rate": 0.00021020466173962476,
      "loss": 0.5353,
      "step": 6320
    },
    {
      "epoch": 0.9011441972851965,
      "grad_norm": 0.5637597441673279,
      "learning_rate": 0.00020992040932347921,
      "loss": 0.5203,
      "step": 6340
    },
    {
      "epoch": 0.9039869234595977,
      "grad_norm": 0.5642934441566467,
      "learning_rate": 0.00020963615690733372,
      "loss": 0.521,
      "step": 6360
    },
    {
      "epoch": 0.906829649633999,
      "grad_norm": 0.602496325969696,
      "learning_rate": 0.00020935190449118817,
      "loss": 0.5333,
      "step": 6380
    },
    {
      "epoch": 0.9096723758084002,
      "grad_norm": 0.5749163627624512,
      "learning_rate": 0.00020906765207504263,
      "loss": 0.5418,
      "step": 6400
    },
    {
      "epoch": 0.9125151019828015,
      "grad_norm": 0.5787551403045654,
      "learning_rate": 0.00020878339965889708,
      "loss": 0.5227,
      "step": 6420
    },
    {
      "epoch": 0.9153578281572028,
      "grad_norm": 0.5964867472648621,
      "learning_rate": 0.00020849914724275156,
      "loss": 0.5147,
      "step": 6440
    },
    {
      "epoch": 0.918200554331604,
      "grad_norm": 0.5750030279159546,
      "learning_rate": 0.000208214894826606,
      "loss": 0.52,
      "step": 6460
    },
    {
      "epoch": 0.9210432805060053,
      "grad_norm": 0.5680714249610901,
      "learning_rate": 0.00020793064241046046,
      "loss": 0.5262,
      "step": 6480
    },
    {
      "epoch": 0.9238860066804065,
      "grad_norm": 0.5923551917076111,
      "learning_rate": 0.00020764638999431494,
      "loss": 0.5298,
      "step": 6500
    },
    {
      "epoch": 0.9267287328548077,
      "grad_norm": 0.5901447534561157,
      "learning_rate": 0.0002073621375781694,
      "loss": 0.5143,
      "step": 6520
    },
    {
      "epoch": 0.929571459029209,
      "grad_norm": 0.5803334712982178,
      "learning_rate": 0.00020707788516202385,
      "loss": 0.5172,
      "step": 6540
    },
    {
      "epoch": 0.9324141852036103,
      "grad_norm": 0.5859044790267944,
      "learning_rate": 0.00020679363274587833,
      "loss": 0.5149,
      "step": 6560
    },
    {
      "epoch": 0.9352569113780115,
      "grad_norm": 0.5708094239234924,
      "learning_rate": 0.00020650938032973278,
      "loss": 0.5097,
      "step": 6580
    },
    {
      "epoch": 0.9380996375524128,
      "grad_norm": 0.6080315113067627,
      "learning_rate": 0.00020622512791358723,
      "loss": 0.522,
      "step": 6600
    },
    {
      "epoch": 0.9409423637268141,
      "grad_norm": 0.6375154256820679,
      "learning_rate": 0.00020594087549744174,
      "loss": 0.5091,
      "step": 6620
    },
    {
      "epoch": 0.9437850899012152,
      "grad_norm": 0.5908357501029968,
      "learning_rate": 0.0002056566230812962,
      "loss": 0.5184,
      "step": 6640
    },
    {
      "epoch": 0.9466278160756165,
      "grad_norm": 0.5805477499961853,
      "learning_rate": 0.00020537237066515064,
      "loss": 0.5287,
      "step": 6660
    },
    {
      "epoch": 0.9494705422500178,
      "grad_norm": 0.5837693214416504,
      "learning_rate": 0.0002050881182490051,
      "loss": 0.5222,
      "step": 6680
    },
    {
      "epoch": 0.952313268424419,
      "grad_norm": 0.5512439012527466,
      "learning_rate": 0.00020480386583285958,
      "loss": 0.5131,
      "step": 6700
    },
    {
      "epoch": 0.9551559945988203,
      "grad_norm": 0.6242421865463257,
      "learning_rate": 0.00020451961341671403,
      "loss": 0.5446,
      "step": 6720
    },
    {
      "epoch": 0.9579987207732216,
      "grad_norm": 0.5520154237747192,
      "learning_rate": 0.00020423536100056848,
      "loss": 0.5069,
      "step": 6740
    },
    {
      "epoch": 0.9608414469476227,
      "grad_norm": 0.574944794178009,
      "learning_rate": 0.00020395110858442296,
      "loss": 0.5156,
      "step": 6760
    },
    {
      "epoch": 0.963684173122024,
      "grad_norm": 0.625889241695404,
      "learning_rate": 0.0002036668561682774,
      "loss": 0.5115,
      "step": 6780
    },
    {
      "epoch": 0.9665268992964253,
      "grad_norm": 0.6042940616607666,
      "learning_rate": 0.00020338260375213187,
      "loss": 0.5209,
      "step": 6800
    },
    {
      "epoch": 0.9693696254708265,
      "grad_norm": 0.6245057582855225,
      "learning_rate": 0.00020309835133598634,
      "loss": 0.5138,
      "step": 6820
    },
    {
      "epoch": 0.9722123516452278,
      "grad_norm": 0.586539089679718,
      "learning_rate": 0.0002028140989198408,
      "loss": 0.5286,
      "step": 6840
    },
    {
      "epoch": 0.9750550778196291,
      "grad_norm": 0.6001536250114441,
      "learning_rate": 0.00020252984650369525,
      "loss": 0.521,
      "step": 6860
    },
    {
      "epoch": 0.9778978039940303,
      "grad_norm": 0.6086289286613464,
      "learning_rate": 0.0002022455940875497,
      "loss": 0.5026,
      "step": 6880
    },
    {
      "epoch": 0.9807405301684315,
      "grad_norm": 0.5853029489517212,
      "learning_rate": 0.0002019613416714042,
      "loss": 0.5218,
      "step": 6900
    },
    {
      "epoch": 0.9835832563428327,
      "grad_norm": 0.5457505583763123,
      "learning_rate": 0.00020167708925525866,
      "loss": 0.5275,
      "step": 6920
    },
    {
      "epoch": 0.986425982517234,
      "grad_norm": 0.5360167622566223,
      "learning_rate": 0.00020139283683911311,
      "loss": 0.5129,
      "step": 6940
    },
    {
      "epoch": 0.9892687086916353,
      "grad_norm": 0.5647407174110413,
      "learning_rate": 0.0002011085844229676,
      "loss": 0.5039,
      "step": 6960
    },
    {
      "epoch": 0.9921114348660365,
      "grad_norm": 0.5975180268287659,
      "learning_rate": 0.00020082433200682205,
      "loss": 0.5214,
      "step": 6980
    },
    {
      "epoch": 0.9949541610404378,
      "grad_norm": 0.6026626825332642,
      "learning_rate": 0.0002005400795906765,
      "loss": 0.5162,
      "step": 7000
    },
    {
      "epoch": 0.9977968872148391,
      "grad_norm": 0.5487368106842041,
      "learning_rate": 0.00020025582717453098,
      "loss": 0.5109,
      "step": 7020
    },
    {
      "epoch": 1.0005685452348803,
      "grad_norm": 0.6921597719192505,
      "learning_rate": 0.00019997157475838543,
      "loss": 0.5129,
      "step": 7040
    },
    {
      "epoch": 1.0034112714092815,
      "grad_norm": 0.5542295575141907,
      "learning_rate": 0.00019968732234223988,
      "loss": 0.5161,
      "step": 7060
    },
    {
      "epoch": 1.0062539975836828,
      "grad_norm": 0.603012204170227,
      "learning_rate": 0.00019940306992609434,
      "loss": 0.5186,
      "step": 7080
    },
    {
      "epoch": 1.009096723758084,
      "grad_norm": 0.5517503619194031,
      "learning_rate": 0.00019911881750994881,
      "loss": 0.5121,
      "step": 7100
    },
    {
      "epoch": 1.0119394499324852,
      "grad_norm": 0.5452717542648315,
      "learning_rate": 0.00019883456509380327,
      "loss": 0.5134,
      "step": 7120
    },
    {
      "epoch": 1.0147821761068865,
      "grad_norm": 0.5953519940376282,
      "learning_rate": 0.00019855031267765772,
      "loss": 0.5072,
      "step": 7140
    },
    {
      "epoch": 1.0176249022812878,
      "grad_norm": 0.57452392578125,
      "learning_rate": 0.00019826606026151223,
      "loss": 0.5178,
      "step": 7160
    },
    {
      "epoch": 1.020467628455689,
      "grad_norm": 0.5864221453666687,
      "learning_rate": 0.00019798180784536668,
      "loss": 0.4943,
      "step": 7180
    },
    {
      "epoch": 1.0233103546300903,
      "grad_norm": 0.6325210332870483,
      "learning_rate": 0.00019769755542922113,
      "loss": 0.5047,
      "step": 7200
    },
    {
      "epoch": 1.0261530808044914,
      "grad_norm": 0.610231876373291,
      "learning_rate": 0.0001974133030130756,
      "loss": 0.5153,
      "step": 7220
    },
    {
      "epoch": 1.0289958069788927,
      "grad_norm": 0.5782414078712463,
      "learning_rate": 0.00019712905059693006,
      "loss": 0.5129,
      "step": 7240
    },
    {
      "epoch": 1.031838533153294,
      "grad_norm": 0.5818278193473816,
      "learning_rate": 0.00019684479818078452,
      "loss": 0.5181,
      "step": 7260
    },
    {
      "epoch": 1.0346812593276953,
      "grad_norm": 0.5540202260017395,
      "learning_rate": 0.00019656054576463897,
      "loss": 0.5126,
      "step": 7280
    },
    {
      "epoch": 1.0375239855020966,
      "grad_norm": 0.6177089214324951,
      "learning_rate": 0.00019627629334849345,
      "loss": 0.5175,
      "step": 7300
    },
    {
      "epoch": 1.0403667116764979,
      "grad_norm": 0.5398880243301392,
      "learning_rate": 0.0001959920409323479,
      "loss": 0.5027,
      "step": 7320
    },
    {
      "epoch": 1.043209437850899,
      "grad_norm": 0.5972217321395874,
      "learning_rate": 0.00019570778851620235,
      "loss": 0.5134,
      "step": 7340
    },
    {
      "epoch": 1.0460521640253002,
      "grad_norm": 0.6057812571525574,
      "learning_rate": 0.00019542353610005683,
      "loss": 0.5056,
      "step": 7360
    },
    {
      "epoch": 1.0488948901997015,
      "grad_norm": 0.5769363045692444,
      "learning_rate": 0.00019513928368391129,
      "loss": 0.5028,
      "step": 7380
    },
    {
      "epoch": 1.0517376163741028,
      "grad_norm": 0.6281857490539551,
      "learning_rate": 0.00019485503126776574,
      "loss": 0.4992,
      "step": 7400
    },
    {
      "epoch": 1.054580342548504,
      "grad_norm": 0.5902922749519348,
      "learning_rate": 0.00019457077885162024,
      "loss": 0.5115,
      "step": 7420
    },
    {
      "epoch": 1.0574230687229054,
      "grad_norm": 0.6181240081787109,
      "learning_rate": 0.0001942865264354747,
      "loss": 0.5012,
      "step": 7440
    },
    {
      "epoch": 1.0602657948973064,
      "grad_norm": 0.5511917471885681,
      "learning_rate": 0.00019400227401932915,
      "loss": 0.4964,
      "step": 7460
    },
    {
      "epoch": 1.0631085210717077,
      "grad_norm": 0.6168341040611267,
      "learning_rate": 0.00019371802160318363,
      "loss": 0.5233,
      "step": 7480
    },
    {
      "epoch": 1.065951247246109,
      "grad_norm": 0.5698665976524353,
      "learning_rate": 0.00019343376918703808,
      "loss": 0.5204,
      "step": 7500
    },
    {
      "epoch": 1.0687939734205103,
      "grad_norm": 0.6030718684196472,
      "learning_rate": 0.00019314951677089253,
      "loss": 0.5023,
      "step": 7520
    },
    {
      "epoch": 1.0716366995949116,
      "grad_norm": 0.5650105476379395,
      "learning_rate": 0.00019286526435474699,
      "loss": 0.5153,
      "step": 7540
    },
    {
      "epoch": 1.0744794257693129,
      "grad_norm": 0.6396806240081787,
      "learning_rate": 0.00019258101193860147,
      "loss": 0.5107,
      "step": 7560
    },
    {
      "epoch": 1.077322151943714,
      "grad_norm": 0.592330813407898,
      "learning_rate": 0.00019229675952245592,
      "loss": 0.5103,
      "step": 7580
    },
    {
      "epoch": 1.0801648781181152,
      "grad_norm": 0.5820307731628418,
      "learning_rate": 0.00019201250710631037,
      "loss": 0.5093,
      "step": 7600
    },
    {
      "epoch": 1.0830076042925165,
      "grad_norm": 0.6023080348968506,
      "learning_rate": 0.00019172825469016485,
      "loss": 0.5142,
      "step": 7620
    },
    {
      "epoch": 1.0858503304669178,
      "grad_norm": 0.6494388580322266,
      "learning_rate": 0.0001914440022740193,
      "loss": 0.5135,
      "step": 7640
    },
    {
      "epoch": 1.088693056641319,
      "grad_norm": 0.5976112484931946,
      "learning_rate": 0.00019115974985787376,
      "loss": 0.5183,
      "step": 7660
    },
    {
      "epoch": 1.0915357828157202,
      "grad_norm": 0.5843673348426819,
      "learning_rate": 0.00019087549744172826,
      "loss": 0.5167,
      "step": 7680
    },
    {
      "epoch": 1.0943785089901215,
      "grad_norm": 0.6044003963470459,
      "learning_rate": 0.00019059124502558271,
      "loss": 0.5178,
      "step": 7700
    },
    {
      "epoch": 1.0972212351645227,
      "grad_norm": 0.5988242030143738,
      "learning_rate": 0.00019030699260943717,
      "loss": 0.5094,
      "step": 7720
    },
    {
      "epoch": 1.100063961338924,
      "grad_norm": 0.6083410382270813,
      "learning_rate": 0.00019002274019329162,
      "loss": 0.4831,
      "step": 7740
    },
    {
      "epoch": 1.1029066875133253,
      "grad_norm": 0.6199246048927307,
      "learning_rate": 0.0001897384877771461,
      "loss": 0.5221,
      "step": 7760
    },
    {
      "epoch": 1.1057494136877266,
      "grad_norm": 0.5959615111351013,
      "learning_rate": 0.00018945423536100055,
      "loss": 0.5128,
      "step": 7780
    },
    {
      "epoch": 1.1085921398621277,
      "grad_norm": 0.5946038365364075,
      "learning_rate": 0.000189169982944855,
      "loss": 0.5116,
      "step": 7800
    },
    {
      "epoch": 1.111434866036529,
      "grad_norm": 0.5890614986419678,
      "learning_rate": 0.00018888573052870948,
      "loss": 0.5035,
      "step": 7820
    },
    {
      "epoch": 1.1142775922109303,
      "grad_norm": 0.5248098969459534,
      "learning_rate": 0.00018860147811256394,
      "loss": 0.503,
      "step": 7840
    },
    {
      "epoch": 1.1171203183853315,
      "grad_norm": 0.6383906006813049,
      "learning_rate": 0.0001883172256964184,
      "loss": 0.5176,
      "step": 7860
    },
    {
      "epoch": 1.1199630445597328,
      "grad_norm": 0.5923133492469788,
      "learning_rate": 0.00018803297328027287,
      "loss": 0.499,
      "step": 7880
    },
    {
      "epoch": 1.1228057707341341,
      "grad_norm": 0.6050794124603271,
      "learning_rate": 0.00018774872086412732,
      "loss": 0.4963,
      "step": 7900
    },
    {
      "epoch": 1.1256484969085352,
      "grad_norm": 0.6149382591247559,
      "learning_rate": 0.00018746446844798177,
      "loss": 0.5027,
      "step": 7920
    },
    {
      "epoch": 1.1284912230829365,
      "grad_norm": 0.6335062384605408,
      "learning_rate": 0.00018718021603183623,
      "loss": 0.5137,
      "step": 7940
    },
    {
      "epoch": 1.1313339492573378,
      "grad_norm": 0.5378568768501282,
      "learning_rate": 0.00018689596361569073,
      "loss": 0.5073,
      "step": 7960
    },
    {
      "epoch": 1.134176675431739,
      "grad_norm": 0.6424155831336975,
      "learning_rate": 0.00018661171119954518,
      "loss": 0.5093,
      "step": 7980
    },
    {
      "epoch": 1.1370194016061403,
      "grad_norm": 0.5795097947120667,
      "learning_rate": 0.00018632745878339964,
      "loss": 0.5174,
      "step": 8000
    },
    {
      "epoch": 1.1398621277805416,
      "grad_norm": 0.6240437030792236,
      "learning_rate": 0.00018604320636725412,
      "loss": 0.512,
      "step": 8020
    },
    {
      "epoch": 1.1427048539549427,
      "grad_norm": 0.6272960901260376,
      "learning_rate": 0.00018575895395110857,
      "loss": 0.5139,
      "step": 8040
    },
    {
      "epoch": 1.145547580129344,
      "grad_norm": 0.6016436815261841,
      "learning_rate": 0.00018547470153496302,
      "loss": 0.5113,
      "step": 8060
    },
    {
      "epoch": 1.1483903063037453,
      "grad_norm": 0.5988212823867798,
      "learning_rate": 0.0001851904491188175,
      "loss": 0.5009,
      "step": 8080
    },
    {
      "epoch": 1.1512330324781466,
      "grad_norm": 0.6211364269256592,
      "learning_rate": 0.00018490619670267195,
      "loss": 0.5041,
      "step": 8100
    },
    {
      "epoch": 1.1540757586525479,
      "grad_norm": 0.5520578026771545,
      "learning_rate": 0.0001846219442865264,
      "loss": 0.4985,
      "step": 8120
    },
    {
      "epoch": 1.156918484826949,
      "grad_norm": 0.5297463536262512,
      "learning_rate": 0.00018433769187038086,
      "loss": 0.5219,
      "step": 8140
    },
    {
      "epoch": 1.1597612110013502,
      "grad_norm": 0.5876452922821045,
      "learning_rate": 0.00018405343945423534,
      "loss": 0.5127,
      "step": 8160
    },
    {
      "epoch": 1.1626039371757515,
      "grad_norm": 0.5479336380958557,
      "learning_rate": 0.00018376918703808982,
      "loss": 0.4972,
      "step": 8180
    },
    {
      "epoch": 1.1654466633501528,
      "grad_norm": 0.628455400466919,
      "learning_rate": 0.00018348493462194427,
      "loss": 0.5077,
      "step": 8200
    },
    {
      "epoch": 1.168289389524554,
      "grad_norm": 0.5957862734794617,
      "learning_rate": 0.00018320068220579875,
      "loss": 0.5151,
      "step": 8220
    },
    {
      "epoch": 1.1711321156989554,
      "grad_norm": 0.6114006042480469,
      "learning_rate": 0.0001829164297896532,
      "loss": 0.5127,
      "step": 8240
    },
    {
      "epoch": 1.1739748418733567,
      "grad_norm": 0.6025121212005615,
      "learning_rate": 0.00018263217737350766,
      "loss": 0.506,
      "step": 8260
    },
    {
      "epoch": 1.1768175680477577,
      "grad_norm": 0.5922427177429199,
      "learning_rate": 0.00018234792495736213,
      "loss": 0.5058,
      "step": 8280
    },
    {
      "epoch": 1.179660294222159,
      "grad_norm": 0.5841841697692871,
      "learning_rate": 0.0001820636725412166,
      "loss": 0.5,
      "step": 8300
    },
    {
      "epoch": 1.1825030203965603,
      "grad_norm": 0.6391715407371521,
      "learning_rate": 0.00018177942012507104,
      "loss": 0.5118,
      "step": 8320
    },
    {
      "epoch": 1.1853457465709616,
      "grad_norm": 0.5420849323272705,
      "learning_rate": 0.00018149516770892552,
      "loss": 0.5158,
      "step": 8340
    },
    {
      "epoch": 1.1881884727453629,
      "grad_norm": 0.6163714528083801,
      "learning_rate": 0.00018121091529277997,
      "loss": 0.5025,
      "step": 8360
    },
    {
      "epoch": 1.191031198919764,
      "grad_norm": 0.6063942909240723,
      "learning_rate": 0.00018092666287663442,
      "loss": 0.5067,
      "step": 8380
    },
    {
      "epoch": 1.1938739250941652,
      "grad_norm": 0.5496679544448853,
      "learning_rate": 0.00018064241046048888,
      "loss": 0.5014,
      "step": 8400
    },
    {
      "epoch": 1.1967166512685665,
      "grad_norm": 0.6003016829490662,
      "learning_rate": 0.00018037237066515063,
      "loss": 0.5191,
      "step": 8420
    },
    {
      "epoch": 1.1995593774429678,
      "grad_norm": 0.6276346445083618,
      "learning_rate": 0.0001800881182490051,
      "loss": 0.5068,
      "step": 8440
    },
    {
      "epoch": 1.202402103617369,
      "grad_norm": 0.592048168182373,
      "learning_rate": 0.00017980386583285956,
      "loss": 0.4945,
      "step": 8460
    },
    {
      "epoch": 1.2052448297917704,
      "grad_norm": 0.592244029045105,
      "learning_rate": 0.00017951961341671402,
      "loss": 0.501,
      "step": 8480
    },
    {
      "epoch": 1.2080875559661717,
      "grad_norm": 0.7241148948669434,
      "learning_rate": 0.0001792353610005685,
      "loss": 0.5155,
      "step": 8500
    },
    {
      "epoch": 1.2109302821405727,
      "grad_norm": 0.6375162601470947,
      "learning_rate": 0.00017895110858442295,
      "loss": 0.5049,
      "step": 8520
    },
    {
      "epoch": 1.213773008314974,
      "grad_norm": 0.5934666395187378,
      "learning_rate": 0.0001786668561682774,
      "loss": 0.509,
      "step": 8540
    },
    {
      "epoch": 1.2166157344893753,
      "grad_norm": 0.5751774907112122,
      "learning_rate": 0.00017838260375213188,
      "loss": 0.4982,
      "step": 8560
    },
    {
      "epoch": 1.2194584606637766,
      "grad_norm": 0.5859614610671997,
      "learning_rate": 0.00017809835133598633,
      "loss": 0.5052,
      "step": 8580
    },
    {
      "epoch": 1.222301186838178,
      "grad_norm": 0.5467990040779114,
      "learning_rate": 0.00017781409891984079,
      "loss": 0.4942,
      "step": 8600
    },
    {
      "epoch": 1.225143913012579,
      "grad_norm": 0.5729141235351562,
      "learning_rate": 0.00017752984650369524,
      "loss": 0.4928,
      "step": 8620
    },
    {
      "epoch": 1.2279866391869803,
      "grad_norm": 0.6200084090232849,
      "learning_rate": 0.00017724559408754975,
      "loss": 0.4979,
      "step": 8640
    },
    {
      "epoch": 1.2308293653613815,
      "grad_norm": 0.5898631811141968,
      "learning_rate": 0.0001769613416714042,
      "loss": 0.5029,
      "step": 8660
    },
    {
      "epoch": 1.2336720915357828,
      "grad_norm": 0.6251531839370728,
      "learning_rate": 0.00017667708925525865,
      "loss": 0.5112,
      "step": 8680
    },
    {
      "epoch": 1.2365148177101841,
      "grad_norm": 0.5632799863815308,
      "learning_rate": 0.00017639283683911313,
      "loss": 0.4895,
      "step": 8700
    },
    {
      "epoch": 1.2393575438845854,
      "grad_norm": 0.5838517546653748,
      "learning_rate": 0.00017610858442296758,
      "loss": 0.5079,
      "step": 8720
    },
    {
      "epoch": 1.2422002700589865,
      "grad_norm": 0.5843129754066467,
      "learning_rate": 0.00017582433200682203,
      "loss": 0.4872,
      "step": 8740
    },
    {
      "epoch": 1.2450429962333878,
      "grad_norm": 0.5946723818778992,
      "learning_rate": 0.00017554007959067651,
      "loss": 0.4973,
      "step": 8760
    },
    {
      "epoch": 1.247885722407789,
      "grad_norm": 0.5887657403945923,
      "learning_rate": 0.00017525582717453097,
      "loss": 0.5063,
      "step": 8780
    },
    {
      "epoch": 1.2507284485821903,
      "grad_norm": 0.5890119075775146,
      "learning_rate": 0.00017497157475838542,
      "loss": 0.522,
      "step": 8800
    },
    {
      "epoch": 1.2535711747565916,
      "grad_norm": 0.5252329111099243,
      "learning_rate": 0.00017468732234223987,
      "loss": 0.5012,
      "step": 8820
    },
    {
      "epoch": 1.2564139009309927,
      "grad_norm": 0.6697691082954407,
      "learning_rate": 0.00017440306992609435,
      "loss": 0.501,
      "step": 8840
    },
    {
      "epoch": 1.259256627105394,
      "grad_norm": 0.6235388517379761,
      "learning_rate": 0.0001741188175099488,
      "loss": 0.5109,
      "step": 8860
    },
    {
      "epoch": 1.2620993532797953,
      "grad_norm": 0.5912278890609741,
      "learning_rate": 0.00017383456509380326,
      "loss": 0.498,
      "step": 8880
    },
    {
      "epoch": 1.2649420794541966,
      "grad_norm": 0.612247884273529,
      "learning_rate": 0.00017355031267765776,
      "loss": 0.5047,
      "step": 8900
    },
    {
      "epoch": 1.2677848056285979,
      "grad_norm": 0.584867000579834,
      "learning_rate": 0.00017326606026151222,
      "loss": 0.5131,
      "step": 8920
    },
    {
      "epoch": 1.2706275318029991,
      "grad_norm": 0.6489502787590027,
      "learning_rate": 0.00017298180784536667,
      "loss": 0.5042,
      "step": 8940
    },
    {
      "epoch": 1.2734702579774004,
      "grad_norm": 0.5241056680679321,
      "learning_rate": 0.00017269755542922115,
      "loss": 0.5143,
      "step": 8960
    },
    {
      "epoch": 1.2763129841518015,
      "grad_norm": 0.6296367645263672,
      "learning_rate": 0.0001724133030130756,
      "loss": 0.4987,
      "step": 8980
    },
    {
      "epoch": 1.2791557103262028,
      "grad_norm": 0.5815621614456177,
      "learning_rate": 0.00017212905059693005,
      "loss": 0.4945,
      "step": 9000
    },
    {
      "epoch": 1.281998436500604,
      "grad_norm": 0.5803189873695374,
      "learning_rate": 0.00017184479818078453,
      "loss": 0.4985,
      "step": 9020
    },
    {
      "epoch": 1.2848411626750054,
      "grad_norm": 0.6107387542724609,
      "learning_rate": 0.00017156054576463898,
      "loss": 0.4999,
      "step": 9040
    },
    {
      "epoch": 1.2876838888494067,
      "grad_norm": 0.5794171094894409,
      "learning_rate": 0.00017127629334849344,
      "loss": 0.4941,
      "step": 9060
    },
    {
      "epoch": 1.2905266150238077,
      "grad_norm": 0.5603388547897339,
      "learning_rate": 0.0001709920409323479,
      "loss": 0.5056,
      "step": 9080
    },
    {
      "epoch": 1.293369341198209,
      "grad_norm": 0.6576698422431946,
      "learning_rate": 0.00017070778851620237,
      "loss": 0.5048,
      "step": 9100
    },
    {
      "epoch": 1.2962120673726103,
      "grad_norm": 0.5562862753868103,
      "learning_rate": 0.00017042353610005682,
      "loss": 0.5057,
      "step": 9120
    },
    {
      "epoch": 1.2990547935470116,
      "grad_norm": 0.5904757380485535,
      "learning_rate": 0.00017013928368391127,
      "loss": 0.4928,
      "step": 9140
    },
    {
      "epoch": 1.3018975197214129,
      "grad_norm": 0.5681484937667847,
      "learning_rate": 0.00016985503126776578,
      "loss": 0.5158,
      "step": 9160
    },
    {
      "epoch": 1.3047402458958142,
      "grad_norm": 0.6150299310684204,
      "learning_rate": 0.00016957077885162023,
      "loss": 0.5233,
      "step": 9180
    },
    {
      "epoch": 1.3075829720702155,
      "grad_norm": 0.6058880686759949,
      "learning_rate": 0.00016928652643547469,
      "loss": 0.484,
      "step": 9200
    },
    {
      "epoch": 1.3104256982446165,
      "grad_norm": 0.5820081233978271,
      "learning_rate": 0.00016900227401932917,
      "loss": 0.4867,
      "step": 9220
    },
    {
      "epoch": 1.3132684244190178,
      "grad_norm": 0.6094101667404175,
      "learning_rate": 0.00016871802160318362,
      "loss": 0.4949,
      "step": 9240
    },
    {
      "epoch": 1.316111150593419,
      "grad_norm": 0.6331531405448914,
      "learning_rate": 0.00016843376918703807,
      "loss": 0.5259,
      "step": 9260
    },
    {
      "epoch": 1.3189538767678204,
      "grad_norm": 0.6134513020515442,
      "learning_rate": 0.00016814951677089252,
      "loss": 0.5134,
      "step": 9280
    },
    {
      "epoch": 1.3217966029422215,
      "grad_norm": 0.5744688510894775,
      "learning_rate": 0.000167865264354747,
      "loss": 0.515,
      "step": 9300
    },
    {
      "epoch": 1.3246393291166227,
      "grad_norm": 0.5584486126899719,
      "learning_rate": 0.00016758101193860145,
      "loss": 0.501,
      "step": 9320
    },
    {
      "epoch": 1.327482055291024,
      "grad_norm": 0.6490016579627991,
      "learning_rate": 0.0001672967595224559,
      "loss": 0.506,
      "step": 9340
    },
    {
      "epoch": 1.3303247814654253,
      "grad_norm": 0.6112567782402039,
      "learning_rate": 0.00016701250710631041,
      "loss": 0.5092,
      "step": 9360
    },
    {
      "epoch": 1.3331675076398266,
      "grad_norm": 0.6498693227767944,
      "learning_rate": 0.00016672825469016487,
      "loss": 0.5123,
      "step": 9380
    },
    {
      "epoch": 1.336010233814228,
      "grad_norm": 0.659191906452179,
      "learning_rate": 0.00016644400227401932,
      "loss": 0.5039,
      "step": 9400
    },
    {
      "epoch": 1.3388529599886292,
      "grad_norm": 0.5544594526290894,
      "learning_rate": 0.0001661597498578738,
      "loss": 0.5073,
      "step": 9420
    },
    {
      "epoch": 1.3416956861630305,
      "grad_norm": 0.6287710666656494,
      "learning_rate": 0.00016587549744172825,
      "loss": 0.5006,
      "step": 9440
    },
    {
      "epoch": 1.3445384123374315,
      "grad_norm": 0.6086244583129883,
      "learning_rate": 0.0001655912450255827,
      "loss": 0.5058,
      "step": 9460
    },
    {
      "epoch": 1.3473811385118328,
      "grad_norm": 0.6502612829208374,
      "learning_rate": 0.00016530699260943716,
      "loss": 0.5006,
      "step": 9480
    },
    {
      "epoch": 1.3502238646862341,
      "grad_norm": 0.6111946105957031,
      "learning_rate": 0.00016502274019329164,
      "loss": 0.5137,
      "step": 9500
    },
    {
      "epoch": 1.3530665908606354,
      "grad_norm": 0.6281488537788391,
      "learning_rate": 0.0001647384877771461,
      "loss": 0.5,
      "step": 9520
    },
    {
      "epoch": 1.3559093170350365,
      "grad_norm": 0.6284810900688171,
      "learning_rate": 0.00016445423536100054,
      "loss": 0.4965,
      "step": 9540
    },
    {
      "epoch": 1.3587520432094378,
      "grad_norm": 0.6264969706535339,
      "learning_rate": 0.00016416998294485502,
      "loss": 0.5069,
      "step": 9560
    },
    {
      "epoch": 1.361594769383839,
      "grad_norm": 0.6153837442398071,
      "learning_rate": 0.00016388573052870947,
      "loss": 0.5021,
      "step": 9580
    },
    {
      "epoch": 1.3644374955582403,
      "grad_norm": 0.5553322434425354,
      "learning_rate": 0.00016360147811256392,
      "loss": 0.5088,
      "step": 9600
    },
    {
      "epoch": 1.3672802217326416,
      "grad_norm": 0.5802475214004517,
      "learning_rate": 0.00016331722569641843,
      "loss": 0.4951,
      "step": 9620
    },
    {
      "epoch": 1.370122947907043,
      "grad_norm": 0.5642178058624268,
      "learning_rate": 0.00016303297328027288,
      "loss": 0.4962,
      "step": 9640
    },
    {
      "epoch": 1.3729656740814442,
      "grad_norm": 0.6410378217697144,
      "learning_rate": 0.00016274872086412734,
      "loss": 0.4861,
      "step": 9660
    },
    {
      "epoch": 1.3758084002558453,
      "grad_norm": 0.5730936527252197,
      "learning_rate": 0.0001624644684479818,
      "loss": 0.5,
      "step": 9680
    },
    {
      "epoch": 1.3786511264302466,
      "grad_norm": 0.6463658213615417,
      "learning_rate": 0.00016218021603183627,
      "loss": 0.5061,
      "step": 9700
    },
    {
      "epoch": 1.3814938526046479,
      "grad_norm": 0.5452410578727722,
      "learning_rate": 0.00016189596361569072,
      "loss": 0.4894,
      "step": 9720
    },
    {
      "epoch": 1.3843365787790491,
      "grad_norm": 0.5343738198280334,
      "learning_rate": 0.00016161171119954517,
      "loss": 0.493,
      "step": 9740
    },
    {
      "epoch": 1.3871793049534504,
      "grad_norm": 0.6136046648025513,
      "learning_rate": 0.00016132745878339965,
      "loss": 0.4844,
      "step": 9760
    },
    {
      "epoch": 1.3900220311278515,
      "grad_norm": 0.5487515330314636,
      "learning_rate": 0.0001610432063672541,
      "loss": 0.4883,
      "step": 9780
    },
    {
      "epoch": 1.3928647573022528,
      "grad_norm": 0.576246440410614,
      "learning_rate": 0.00016075895395110856,
      "loss": 0.4885,
      "step": 9800
    }
  ],
  "logging_steps": 20,
  "max_steps": 21108,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 200,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 4.990167486604247e+17,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
